{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "HdfsError",
     "evalue": "Remote path '/user/anna/flight_data/Flight_Delay.parquet' already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHdfsError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m hdfs_destination_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/user/anna/flight_data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Upload the local file to HDFS\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m hdfs_client\u001b[39m.\u001b[39;49mupload(hdfs_destination_path, local_file_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mlocal_file_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m uploaded to HDFS at \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mhdfs_destination_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/hdfs/client.py:599\u001b[0m, in \u001b[0;36mClient.upload\u001b[0;34m(self, hdfs_path, local_path, n_threads, temp_dir, chunk_size, progress, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[39mif\u001b[39;00m local_name \u001b[39min\u001b[39;00m suffixes:\n\u001b[1;32m    598\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 599\u001b[0m     \u001b[39mraise\u001b[39;00m HdfsError(\u001b[39m'\u001b[39m\u001b[39mRemote path \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m already exists.\u001b[39m\u001b[39m'\u001b[39m, hdfs_path)\n\u001b[1;32m    600\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    601\u001b[0m   temp_path \u001b[39m=\u001b[39m hdfs_path\n",
      "\u001b[0;31mHdfsError\u001b[0m: Remote path '/user/anna/flight_data/Flight_Delay.parquet' already exists."
     ]
    }
   ],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "# Connect to HDFS (adjust the URL to match your Hadoop cluster)\n",
    "hdfs_client = InsecureClient(\"http://localhost:9870\", user=\"anna\")\n",
    "\n",
    "# Local file path\n",
    "local_file_path = \"/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/data/Flight_data/Flight_Delay.parquet\"\n",
    "\n",
    "# HDFS destination path\n",
    "hdfs_destination_path = \"/user/anna/flight_data\"\n",
    "\n",
    "# Upload the local file to HDFS\n",
    "hdfs_client.upload(hdfs_destination_path, local_file_path)\n",
    "\n",
    "print(f\"File '{local_file_path}' uploaded to HDFS at '{hdfs_destination_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "HdfsError",
     "evalue": "Remote path '/user/anna/flight_data/features_added.parquet' already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHdfsError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m hdfs_destination_path2 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/user/anna/flight_data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Upload the local file to HDFS\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m hdfs_client\u001b[39m.\u001b[39;49mupload(hdfs_destination_path2, local_file_path2)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mlocal_file_path2\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m uploaded to HDFS at \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mhdfs_destination_path2\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/hdfs/client.py:599\u001b[0m, in \u001b[0;36mClient.upload\u001b[0;34m(self, hdfs_path, local_path, n_threads, temp_dir, chunk_size, progress, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[39mif\u001b[39;00m local_name \u001b[39min\u001b[39;00m suffixes:\n\u001b[1;32m    598\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 599\u001b[0m     \u001b[39mraise\u001b[39;00m HdfsError(\u001b[39m'\u001b[39m\u001b[39mRemote path \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m already exists.\u001b[39m\u001b[39m'\u001b[39m, hdfs_path)\n\u001b[1;32m    600\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    601\u001b[0m   temp_path \u001b[39m=\u001b[39m hdfs_path\n",
      "\u001b[0;31mHdfsError\u001b[0m: Remote path '/user/anna/flight_data/features_added.parquet' already exists."
     ]
    }
   ],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "# Connect to HDFS (adjust the URL to match your Hadoop cluster)\n",
    "hdfs_client = InsecureClient(\"http://localhost:9870\", user=\"anna\")\n",
    "\n",
    "# Local file path\n",
    "local_file_path2 = \"/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/data/Flight_data/features_added.parquet\"\n",
    "\n",
    "# HDFS destination path\n",
    "hdfs_destination_path2 = \"/user/anna/flight_data\"\n",
    "\n",
    "# Upload the local file to HDFS\n",
    "hdfs_client.upload(hdfs_destination_path2, local_file_path2)\n",
    "\n",
    "print(f\"File '{local_file_path2}' uploaded to HDFS at '{hdfs_destination_path2}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/13 19:23:22 WARN Utils: Your hostname, LAPTOP-PCQSOCPF resolves to a loopback address: 127.0.1.1; using 172.24.131.235 instead (on interface eth0)\n",
      "23/09/13 19:23:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/13 19:23:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/13 19:24:02 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----------+-------------------------+--------------+--------------+----------+-------+--------+---------------+-------+---------+--------+------+----------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+-----------------+\n",
      "|Year|Month|DayofMonth|FlightDate|Marketing_Airline_Network|OriginCityName|  DestCityName|CRSDepTime|DepTime|DepDelay|DepDelayMinutes|TaxiOut|WheelsOff|WheelsOn|TaxiIn|CRSArrTime|ArrTime|ArrDelay|ArrDelayMinutes|CRSElapsedTime|ActualElapsedTime|AirTime|Distance|DistanceGroup|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|__index_level_0__|\n",
      "+----+-----+----------+----------+-------------------------+--------------+--------------+----------+-------+--------+---------------+-------+---------+--------+------+----------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+-----------------+\n",
      "|2018|    1|        15|2018-01-15|                       UA|    Newark, NJ|Charleston, SC|      1845| 1928.0|    43.0|           43.0|   35.0|   2003.0|  2145.0|   4.0|      2108| 2149.0|    41.0|           41.0|         143.0|            141.0|  102.0|   628.0|            3|        41.0|         0.0|     0.0|          0.0|              0.0|                1|\n",
      "|2018|    1|        16|2018-01-16|                       UA|    Newark, NJ|Charleston, SC|      1835| 1956.0|    81.0|           81.0|   18.0|   2014.0|  2202.0|   5.0|      2058| 2207.0|    69.0|           69.0|         143.0|            131.0|  108.0|   628.0|            3|        67.0|         0.0|     0.0|          0.0|              2.0|                2|\n",
      "|2018|    1|        17|2018-01-17|                       UA|    Newark, NJ|Charleston, SC|      1835| 1836.0|     1.0|            1.0|   19.0|   1855.0|  2048.0|   6.0|      2058| 2054.0|    -4.0|            0.0|         143.0|            138.0|  113.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|                3|\n",
      "|2018|    1|        18|2018-01-18|                       UA|    Newark, NJ|Charleston, SC|      1845| 1844.0|    -1.0|            0.0|   36.0|   1920.0|  2052.0|   6.0|      2108| 2058.0|   -10.0|            0.0|         143.0|            134.0|   92.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|                4|\n",
      "|2018|    1|        20|2018-01-20|                       UA|    Newark, NJ|Charleston, SC|      1835| 1829.0|    -6.0|            0.0|   22.0|   1851.0|  2020.0|   6.0|      2058| 2026.0|   -32.0|            0.0|         143.0|            117.0|   89.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|                6|\n",
      "+----+-----+----------+----------+-------------------------+--------------+--------------+----------+-------+--------+---------------+-------+---------+--------+------+----------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "# Connect to HDFS (adjust the URL to match your Hadoop cluster)\n",
    "hdfs_client = InsecureClient(\"http://localhost:9870\", user=\"anna\")\n",
    "\n",
    "# Create a SparkConf object and set Hadoop configuration properties\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"MySparkApp\") \\\n",
    "    .setMaster(\"local[2]\")  # Use your Spark cluster's master URL if not running locally\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.defaultFS\", \"http://localhost:9870\")\n",
    "conf.set(\"spark.hadoop.yarn.resourcemanager.hostname\", \"http://localhost:8088/\")\n",
    "\n",
    "# Create a SparkSession with the configured conf object\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Now you can use Spark to access HDFS\n",
    "parquet_file_path = \"hdfs://localhost:9000/user/anna/flight_data/Flight_Delay.parquet\"\n",
    "df = spark.read.parquet(parquet_file_path)\n",
    "df.show(5)\n",
    "\n",
    "# # Now you can use Spark to access HDFS\n",
    "# parquet_file_path2 = \"hdfs://localhost:9000//user/anna/flight_data/features_added.parquet\"\n",
    "# df2 = spark.read.parquet(parquet_file_path2)\n",
    "# df2.show(5)\n",
    "\n",
    "# Don't forget to stop the SparkSession when done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/17 12:00:13 WARN Utils: Your hostname, LAPTOP-PCQSOCPF resolves to a loopback address: 127.0.1.1; using 172.30.62.254 instead (on interface eth0)\n",
      "23/10/17 12:00:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/17 12:00:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/10/17 12:15:02 ERROR SparkContext: Error initializing SparkContext.\n",
      "org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\n",
      "\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)\n",
      "23/10/17 12:15:02 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to send shutdown message before the AM has registered!\n",
      "23/10/17 12:15:02 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "23/10/17 12:15:02 WARN MetricsSystem: Stopping a MetricsSystem that is not running\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m conf\u001b[39m.\u001b[39mset(\u001b[39m\"\u001b[39m\u001b[39mspark.hadoop.yarn.resourcemanager.hostname\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhttp://localhost:8088/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Create a SparkContext with the configured conf object\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m sc \u001b[39m=\u001b[39m SparkContext(conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Create a SparkSession (optional, for Spark SQL)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/pyspark/context.py:200\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    198\u001b[0m SparkContext\u001b[39m.\u001b[39m_ensure_initialized(\u001b[39mself\u001b[39m, gateway\u001b[39m=\u001b[39mgateway, conf\u001b[39m=\u001b[39mconf)\n\u001b[1;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_init(\n\u001b[1;32m    201\u001b[0m         master,\n\u001b[1;32m    202\u001b[0m         appName,\n\u001b[1;32m    203\u001b[0m         sparkHome,\n\u001b[1;32m    204\u001b[0m         pyFiles,\n\u001b[1;32m    205\u001b[0m         environment,\n\u001b[1;32m    206\u001b[0m         batchSize,\n\u001b[1;32m    207\u001b[0m         serializer,\n\u001b[1;32m    208\u001b[0m         conf,\n\u001b[1;32m    209\u001b[0m         jsc,\n\u001b[1;32m    210\u001b[0m         profiler_cls,\n\u001b[1;32m    211\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    213\u001b[0m     )\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[39m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop()\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/pyspark/context.py:287\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment[\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    286\u001b[0m \u001b[39m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc \u001b[39m=\u001b[39m jsc \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_context(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conf\u001b[39m.\u001b[39;49m_jconf)\n\u001b[1;32m    288\u001b[0m \u001b[39m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf \u001b[39m=\u001b[39m SparkConf(_jconf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39mconf())\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/pyspark/context.py:417\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[39mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mJavaSparkContext(jconf)\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1588\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[1;32m   1590\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "# Connect to HDFS (adjust the URL to match your Hadoop cluster)\n",
    "hdfs_client = InsecureClient(\"http://localhost:9870\", user=\"anna\")\n",
    "\n",
    "# Create a SparkConf object\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"MySparkApp\") \\\n",
    "    .setMaster(\"yarn\")  # Use the cluster manager you're using (e.g., yarn, mesos, etc.)\n",
    "\n",
    "# Configure the number of executors, executor memory, and other properties\n",
    "conf.set(\"spark.executor.instances\", \"4\")  # Number of executor instances\n",
    "conf.set(\"spark.executor.memory\", \"2g\")     # Memory per executor\n",
    "conf.set(\"spark.executor.cores\", \"2\")       # Cores per executor\n",
    "conf.set(\"spark.driver.memory\", \"2g\")       # Memory for the driver (main application)\n",
    "conf.set(\"spark.hadoop.fs.defaultFS\", \"http://localhost:9870\")\n",
    "conf.set(\"spark.hadoop.yarn.resourcemanager.hostname\", \"http://localhost:8088/\")\n",
    "\n",
    "# Create a SparkContext with the configured conf object\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Create a SparkSession (optional, for Spark SQL)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Now you can use Spark for processing\n",
    "\n",
    "# Now you can use Spark to access HDFS\n",
    "parquet_file_path = \"hdfs://localhost:9000/user/anna/flight_data/Flight_Delay.parquet\"\n",
    "df = spark.read.parquet(parquet_file_path)\n",
    "df.show(5)\n",
    "\n",
    "# Now you can use Spark to access HDFS\n",
    "parquet_file_path2 = \"hdfs://localhost:9000//user/anna/flight_data/features_added.parquet\"\n",
    "df2 = spark.read.parquet(parquet_file_path2)\n",
    "df2.show(5)\n",
    "\n",
    "# Don't forget to stop the SparkSession when done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/13 19:57:27 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-------------------+-------------------------+--------------+--------------+-------+--------+---------------+-------+------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+---------+--------+----------------+--------------+---------------+-------------+----------------+--------------+--------------+------------+-----------------+----------------+-----------------+---------------+-------------------+-----------------+\n",
      "|Year|Month|DayofMonth|         FlightDate|Marketing_Airline_Network|OriginCityName|  DestCityName|DepTime|DepDelay|DepDelayMinutes|TaxiOut|TaxiIn|ArrTime|ArrDelay|ArrDelayMinutes|CRSElapsedTime|ActualElapsedTime|AirTime|Distance|DistanceGroup|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|DayofWeek|Holidays|CRSDepTimeMinute|CRSDepTimeHour|WheelsOffMinute|WheelsOffHour|CRSArrTimeMinute|CRSArrTimeHour|WheelsOnMinute|WheelsOnHour|CRSDepTimeHourDis|WheelsOffHourDis|CRSArrTimeHourDis|WheelsOnHourDis|CRSElapsedTimeGorup|__index_level_0__|\n",
      "+----+-----+----------+-------------------+-------------------------+--------------+--------------+-------+--------+---------------+-------+------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+---------+--------+----------------+--------------+---------------+-------------+----------------+--------------+--------------+------------+-----------------+----------------+-----------------+---------------+-------------------+-----------------+\n",
      "|2018|    1|        15|2018-01-15 07:00:00|                       UA|    Newark, NJ|Charleston, SC| 1928.0|    43.0|           43.0|   35.0|   4.0| 2149.0|    41.0|           41.0|         143.0|            141.0|  102.0|   628.0|            3|        41.0|         0.0|     0.0|          0.0|              0.0|        0|    true|              45|            18|              3|           20|               8|            21|            45|          21|          Evening|         Evening|            Night|          Night|                  3|                1|\n",
      "|2018|    1|        16|2018-01-16 07:00:00|                       UA|    Newark, NJ|Charleston, SC| 1956.0|    81.0|           81.0|   18.0|   5.0| 2207.0|    69.0|           69.0|         143.0|            131.0|  108.0|   628.0|            3|        67.0|         0.0|     0.0|          0.0|              2.0|        1|   false|              35|            18|             14|           20|              58|            20|             2|          22|          Evening|         Evening|          Evening|          Night|                  3|                2|\n",
      "|2018|    1|        17|2018-01-17 07:00:00|                       UA|    Newark, NJ|Charleston, SC| 1836.0|     1.0|            1.0|   19.0|   6.0| 2054.0|    -4.0|            0.0|         143.0|            138.0|  113.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|        2|   false|              35|            18|             55|           18|              58|            20|            48|          20|          Evening|         Evening|          Evening|        Evening|                  3|                3|\n",
      "|2018|    1|        18|2018-01-18 07:00:00|                       UA|    Newark, NJ|Charleston, SC| 1844.0|    -1.0|            0.0|   36.0|   6.0| 2058.0|   -10.0|            0.0|         143.0|            134.0|   92.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|        3|   false|              45|            18|             20|           19|               8|            21|            52|          20|          Evening|         Evening|            Night|        Evening|                  3|                4|\n",
      "|2018|    1|        20|2018-01-20 07:00:00|                       UA|    Newark, NJ|Charleston, SC| 1829.0|    -6.0|            0.0|   22.0|   6.0| 2026.0|   -32.0|            0.0|         143.0|            117.0|   89.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|        5|   false|              35|            18|             51|           18|              58|            20|            20|          20|          Evening|         Evening|          Evening|        Evening|                  3|                6|\n",
      "+----+-----+----------+-------------------+-------------------------+--------------+--------------+-------+--------+---------------+-------+------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+---------+--------+----------------+--------------+---------------+-------------+----------------+--------------+--------------+------------+-----------------+----------------+-----------------+---------------+-------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "# Connect to HDFS (adjust the URL to match your Hadoop cluster)\n",
    "hdfs_client = InsecureClient(\"http://localhost:9870\", user=\"anna\")\n",
    "\n",
    "# Create a SparkConf object and set Hadoop configuration properties\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"MySparkApp2\") \\\n",
    "    .setMaster(\"local[2]\")  # Use your Spark cluster's master URL if not running locally\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.defaultFS\", \"http://localhost:9870\")\n",
    "conf.set(\"spark.hadoop.yarn.resourcemanager.hostname\", \"http://localhost:8088/\")\n",
    "\n",
    "# Create a SparkSession with the configured conf object\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Now you can use Spark to access HDFS\n",
    "parquet_file_path2 = \"hdfs://localhost:9000//user/anna/flight_data/features_added.parquet\"\n",
    "df2 = spark.read.parquet(parquet_file_path2)\n",
    "df2.show(5)\n",
    "\n",
    "# Don't forget to stop the SparkSession when done\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Month: long (nullable = true)\n",
      " |-- DayofMonth: long (nullable = true)\n",
      " |-- FlightDate: string (nullable = true)\n",
      " |-- Marketing_Airline_Network: string (nullable = true)\n",
      " |-- OriginCityName: string (nullable = true)\n",
      " |-- DestCityName: string (nullable = true)\n",
      " |-- CRSDepTime: long (nullable = true)\n",
      " |-- DepTime: double (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- DepDelayMinutes: double (nullable = true)\n",
      " |-- TaxiOut: double (nullable = true)\n",
      " |-- WheelsOff: double (nullable = true)\n",
      " |-- WheelsOn: double (nullable = true)\n",
      " |-- TaxiIn: double (nullable = true)\n",
      " |-- CRSArrTime: long (nullable = true)\n",
      " |-- ArrTime: double (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- ArrDelayMinutes: double (nullable = true)\n",
      " |-- CRSElapsedTime: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- DistanceGroup: long (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "# Connect to HDFS (adjust the URL to match your Hadoop cluster)\n",
    "hdfs_client = InsecureClient(\"http://localhost:9870\", user=\"anna\")\n",
    "\n",
    "# Create a SparkConf object and set Hadoop configuration properties\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"MySparkApp\") \\\n",
    "    .setMaster(\"local[2]\")  # Use your Spark cluster's master URL if not running locally\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\")\n",
    "conf.set(\"spark.hadoop.yarn.resourcemanager.hostname\", \"localhost\")\n",
    "\n",
    "# Create a SparkSession with the configured conf object\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Now you can use Spark to access HDFS\n",
    "parquet_file_path = \"hdfs://localhost:9000/user/anna/flight_data/Flight_Delay.parquet\"\n",
    "\n",
    "# Read the Parquet file\n",
    "df = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Print the schema (column names and data types)\n",
    "df.printSchema()\n",
    "\n",
    "# Don't forget to stop the SparkSession when done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Month: long (nullable = true)\n",
      " |-- DayofMonth: long (nullable = true)\n",
      " |-- FlightDate: timestamp_ntz (nullable = true)\n",
      " |-- Marketing_Airline_Network: string (nullable = true)\n",
      " |-- OriginCityName: string (nullable = true)\n",
      " |-- DestCityName: string (nullable = true)\n",
      " |-- DepTime: double (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- DepDelayMinutes: double (nullable = true)\n",
      " |-- TaxiOut: double (nullable = true)\n",
      " |-- TaxiIn: double (nullable = true)\n",
      " |-- ArrTime: double (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- ArrDelayMinutes: double (nullable = true)\n",
      " |-- CRSElapsedTime: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- DistanceGroup: long (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      " |-- DayofWeek: long (nullable = true)\n",
      " |-- Holidays: boolean (nullable = true)\n",
      " |-- CRSDepTimeMinute: integer (nullable = true)\n",
      " |-- CRSDepTimeHour: integer (nullable = true)\n",
      " |-- WheelsOffMinute: integer (nullable = true)\n",
      " |-- WheelsOffHour: integer (nullable = true)\n",
      " |-- CRSArrTimeMinute: integer (nullable = true)\n",
      " |-- CRSArrTimeHour: integer (nullable = true)\n",
      " |-- WheelsOnMinute: integer (nullable = true)\n",
      " |-- WheelsOnHour: integer (nullable = true)\n",
      " |-- CRSDepTimeHourDis: string (nullable = true)\n",
      " |-- WheelsOffHourDis: string (nullable = true)\n",
      " |-- CRSArrTimeHourDis: string (nullable = true)\n",
      " |-- WheelsOnHourDis: string (nullable = true)\n",
      " |-- CRSElapsedTimeGorup: long (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "# Connect to HDFS (adjust the URL to match your Hadoop cluster)\n",
    "hdfs_client = InsecureClient(\"http://localhost:9870\", user=\"anna\")\n",
    "\n",
    "# Create a SparkConf object and set Hadoop configuration properties\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"MySparkApp\") \\\n",
    "    .setMaster(\"local[2]\")  # Use your Spark cluster's master URL if not running locally\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\")\n",
    "conf.set(\"spark.hadoop.yarn.resourcemanager.hostname\", \"localhost\")\n",
    "\n",
    "# Create a SparkSession with the configured conf object\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Now you can use Spark to access HDFS\n",
    "parquet_file_path2 = \"hdfs://localhost:9000/user/anna/flight_data/features_added.parquet\"\n",
    "\n",
    "# Read the Parquet file\n",
    "df2 = spark.read.parquet(parquet_file_path2)\n",
    "\n",
    "# Print the schema (column names and data types)\n",
    "df2.printSchema()\n",
    "\n",
    "# Don't forget to stop the SparkSession when done\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

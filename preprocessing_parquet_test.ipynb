{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from spark_hdfs import SparkHDFSConnector, SparkHDFSReader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat, col, lit, sum, avg, when, count, date_format, isnan, log, expr, unix_timestamp\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Month: long (nullable = true)\n",
      " |-- DayofMonth: long (nullable = true)\n",
      " |-- FlightDate: timestamp (nullable = true)\n",
      " |-- Marketing_Airline_Network: string (nullable = true)\n",
      " |-- OriginCityName: string (nullable = true)\n",
      " |-- DestCityName: string (nullable = true)\n",
      " |-- DepTime: double (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- DepDelayMinutes: double (nullable = true)\n",
      " |-- TaxiOut: double (nullable = true)\n",
      " |-- TaxiIn: double (nullable = true)\n",
      " |-- ArrTime: double (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- ArrDelayMinutes: double (nullable = true)\n",
      " |-- CRSElapsedTime: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- DistanceGroup: long (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      " |-- DayofWeek: long (nullable = true)\n",
      " |-- Holidays: boolean (nullable = true)\n",
      " |-- CRSDepTimeMinute: integer (nullable = true)\n",
      " |-- CRSDepTimeHour: integer (nullable = true)\n",
      " |-- WheelsOffMinute: integer (nullable = true)\n",
      " |-- WheelsOffHour: integer (nullable = true)\n",
      " |-- CRSArrTimeMinute: integer (nullable = true)\n",
      " |-- CRSArrTimeHour: integer (nullable = true)\n",
      " |-- WheelsOnMinute: integer (nullable = true)\n",
      " |-- WheelsOnHour: integer (nullable = true)\n",
      " |-- CRSDepTimeHourDis: string (nullable = true)\n",
      " |-- WheelsOffHourDis: string (nullable = true)\n",
      " |-- CRSArrTimeHourDis: string (nullable = true)\n",
      " |-- WheelsOnHourDis: string (nullable = true)\n",
      " |-- CRSElapsedTimeGorup: long (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: DepTime, Lower Bound: -143.5, Upper Bound: 2732.5\n",
      "Column: DepDelay, Lower Bound: -19.5, Upper Bound: 16.5\n",
      "Column: TaxiOut, Lower Bound: -1.0, Upper Bound: 31.0\n",
      "Column: TaxiIn, Lower Bound: -3.5, Upper Bound: 16.5\n",
      "Column: ArrTime, Lower Bound: -22.5, Upper Bound: 2981.5\n",
      "Column: ArrDelay, Lower Bound: -39.5, Upper Bound: 20.5\n",
      "Column: CRSElapsedTime, Lower Bound: -25.0, Upper Bound: 279.0\n",
      "Column: ActualElapsedTime, Lower Bound: -37.0, Upper Bound: 275.0\n",
      "Column: AirTime, Lower Bound: -51.0, Upper Bound: 245.0\n",
      "Column: Distance, Lower Bound: -515.5, Upper Bound: 1840.5\n",
      "Column: CarrierDelay, Lower Bound: 0.0, Upper Bound: 0.0\n",
      "Column: WeatherDelay, Lower Bound: 0.0, Upper Bound: 0.0\n",
      "Column: NASDelay, Lower Bound: 0.0, Upper Bound: 0.0\n",
      "Column: SecurityDelay, Lower Bound: 0.0, Upper Bound: 0.0\n",
      "Column: LateAircraftDelay, Lower Bound: 0.0, Upper Bound: 0.0\n",
      "Column: CRSDepTimeMinute, Lower Bound: -30.0, Upper Bound: 90.0\n",
      "Column: CRSDepTimeHour, Lower Bound: -3.0, Upper Bound: 29.0\n",
      "Column: WheelsOffMinute, Lower Bound: -28.0, Upper Bound: 84.0\n",
      "Column: WheelsOffHour, Lower Bound: -4.5, Upper Bound: 31.5\n",
      "Column: CRSArrTimeMinute, Lower Bound: -28.5, Upper Bound: 87.5\n",
      "Column: CRSArrTimeHour, Lower Bound: -1.0, Upper Bound: 31.0\n",
      "Column: WheelsOnMinute, Lower Bound: -28.5, Upper Bound: 87.5\n",
      "Column: WheelsOnHour, Lower Bound: -1.0, Upper Bound: 31.0\n",
      "23/10/12 15:42:21 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-------------------+-------------------------+--------------+--------------+-------+------------------+---------------+-------+------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+---------+--------+----------------+--------------+---------------+-------------+----------------+--------------+--------------+------------+-----------------+----------------+-----------------+---------------+-------------------+-----------------+-------------------+\n",
      "|Year|Month|DayofMonth|         FlightDate|Marketing_Airline_Network|OriginCityName|  DestCityName|DepTime|          DepDelay|DepDelayMinutes|TaxiOut|TaxiIn|ArrTime|ArrDelay|ArrDelayMinutes|CRSElapsedTime|ActualElapsedTime|AirTime|Distance|DistanceGroup|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|DayofWeek|Holidays|CRSDepTimeMinute|CRSDepTimeHour|WheelsOffMinute|WheelsOffHour|CRSArrTimeMinute|CRSArrTimeHour|WheelsOnMinute|WheelsOnHour|CRSDepTimeHourDis|WheelsOffHourDis|CRSArrTimeHourDis|WheelsOnHourDis|CRSElapsedTimeGorup|__index_level_0__|         TotalDelay|\n",
      "+----+-----+----------+-------------------+-------------------------+--------------+--------------+-------+------------------+---------------+-------+------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+---------+--------+----------------+--------------+---------------+-------------+----------------+--------------+--------------+------------+-----------------+----------------+-----------------+---------------+-------------------+-----------------+-------------------+\n",
      "|2018|    1|        17|2018-01-17 07:00:00|                       UA|    Newark, NJ|Charleston, SC| 1836.0|0.6931471805599453|            1.0|   19.0|   6.0| 2054.0|    -4.0|            0.0|         143.0|            138.0|  113.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|        2|   false|              35|            18|             55|           18|              58|            20|            48|          20|          Evening|         Evening|          Evening|        Evening|                  3|                3|-3.3068528194400546|\n",
      "|2018|    1|        20|2018-01-20 07:00:00|                       UA|    Newark, NJ|Charleston, SC| 1829.0|              null|            0.0|   22.0|   6.0| 2026.0|   -32.0|            0.0|         143.0|            117.0|   89.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|        5|   false|              35|            18|             51|           18|              58|            20|            20|          20|          Evening|         Evening|          Evening|        Evening|                  3|                6|               null|\n",
      "|2018|    1|        27|2018-01-27 07:00:00|                       UA|    Newark, NJ|Charleston, SC| 1825.0|              null|            0.0|   15.0|   5.0| 2026.0|   -32.0|            0.0|         143.0|            121.0|  101.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|        5|   false|              35|            18|             40|           18|              58|            20|            21|          20|          Evening|         Evening|          Evening|        Evening|                  3|               13|               null|\n",
      "|2018|    1|        28|2018-01-28 07:00:00|                       UA|    Newark, NJ|Charleston, SC| 1828.0|              null|            0.0|   16.0|   6.0| 2040.0|   -18.0|            0.0|         143.0|            132.0|  110.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|        6|   false|              35|            18|             44|           18|              58|            20|            34|          20|          Evening|         Evening|          Evening|        Evening|                  3|               14|               null|\n",
      "|2018|    1|        30|2018-01-30 07:00:00|                       UA|    Newark, NJ|Charleston, SC| 1829.0|              null|            0.0|   19.0|   9.0| 2026.0|   -32.0|            0.0|         143.0|            117.0|   89.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|        1|   false|              35|            18|             48|           18|              58|            20|            17|          20|          Evening|         Evening|          Evening|        Evening|                  3|               16|               null|\n",
      "+----+-----+----------+-------------------+-------------------------+--------------+--------------+-------+------------------+---------------+-------+------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+---------+--------+----------------+--------------+---------------+-------------+----------------+--------------+--------------+------------+-----------------+----------------+-----------------+---------------+-------------------+-----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    hdfs_url = \"http://localhost:9870\"\n",
    "    hdfs_user = \"anna\"\n",
    "    spark_app_name = \"ReadParquetAppSpark\"\n",
    "    spark_master_url = \"local[2]\"  # Use Spark cluster's master URL if not running locally\n",
    "\n",
    "    # Create an instance of the SparkHDFSConnector class\n",
    "    connector = SparkHDFSConnector(hdfs_url, hdfs_user, spark_app_name, spark_master_url)\n",
    "\n",
    "    # Define the Parquet file path\n",
    "    parquet_file_path = \"hdfs://localhost:9000/user/anna/flight_data/features_added.parquet\"\n",
    "\n",
    "    # Read and show the Parquet file using the connector\n",
    "    df = connector.read_parquet_from_hdfs(parquet_file_path)\n",
    "\n",
    "    # Stop the Spark session for the connector\n",
    "    connector.stop_spark_session()\n",
    "\n",
    "    # Create an instance of the SparkHDFSReader class\n",
    "    hdfs_reader = SparkHDFSReader(hdfs_url, hdfs_user)\n",
    "\n",
    "    # Create a Spark session for other operations\n",
    "    hdfs_reader.create_hdfs_spark_session(app_name=\"ParquetPreprocessingApp\")\n",
    "\n",
    "    # Read and print the schema for the Parquet file using the reader\n",
    "    df = hdfs_reader.read_parquet_file(parquet_file_path)\n",
    "\n",
    "    #hdfs_reader.show_dataframe(df)\n",
    "    hdfs_reader.print_schema(df)\n",
    "\n",
    "    # Load the Parquet data\n",
    "    parquet_data = hdfs_reader.read_parquet_file(parquet_file_path)\n",
    "\n",
    "    # Check for missing values in the DataFrame\n",
    "    missing_values = parquet_data.select([count(when(parquet_data[c].isNull(), c)).alias(c) for c in parquet_data.columns])\n",
    "\n",
    "    # Define the threshold percentage (e.g., 30%)\n",
    "    threshold_percentage = 30\n",
    "\n",
    "    # Calculate the total number of rows in the DataFrame\n",
    "    total_rows = parquet_data.count()\n",
    "\n",
    "    # Calculate the percentage of missing values for each column\n",
    "    missing_percentage = [(col_name, (total_rows - parquet_data.filter(col(col_name).isNotNull()).count()) / total_rows * 100) for col_name in parquet_data.columns]\n",
    "\n",
    "    # Determine columns with missing values exceeding the threshold\n",
    "    columns_to_drop = [col_name for col_name, percentage in missing_percentage if percentage > threshold_percentage]\n",
    "\n",
    "    # Drop the identified columns with excessive missing data\n",
    "    parquet_data = parquet_data.drop(*columns_to_drop)\n",
    "\n",
    "    # To drop rows with missing values in specific columns, you can use the drop method\n",
    "    # For example, to drop rows with missing values in the \"DepDelay\" and \"ArrDelay\" columns:\n",
    "    columns_to_drop = [\"DepDelay\", \"ArrDelay\"]\n",
    "    parquet_data = parquet_data.dropna(subset=columns_to_drop)\n",
    "\n",
    "    # To impute missing values with a specific value (e.g., 0) in specific columns:\n",
    "    columns_to_impute = [\"DepDelay\", \"ArrDelay\"]\n",
    "    for column in columns_to_impute:\n",
    "        parquet_data = parquet_data.fillna(0, subset=column)\n",
    "\n",
    "    #Select the numeric columns you want to analyze for outliers.\n",
    "    numeric_columns = [\"DepTime\", \"DepDelay\", \"TaxiOut\", \"TaxiIn\", \"ArrTime\", \"ArrDelay\", \"CRSElapsedTime\", \"ActualElapsedTime\", \"AirTime\", \n",
    "                   \"Distance\", \"CarrierDelay\", \"WeatherDelay\" , \"NASDelay\" , \"SecurityDelay\", \"LateAircraftDelay\", \"CRSDepTimeMinute\", \n",
    "                   \"CRSDepTimeHour\", \"WheelsOffMinute\" , \"WheelsOffHour\", \"CRSArrTimeMinute\", \"CRSArrTimeHour\", \"WheelsOnMinute\", \n",
    "                   \"WheelsOnHour\" ]  # Replace with column names\n",
    "    df = df.select(*numeric_columns)\n",
    "\n",
    "    def calculate_iqr_bounds(df, columns):\n",
    "        bounds = {}\n",
    "        for col_name in columns:\n",
    "            quantiles = df.approxQuantile(col_name, [0.25, 0.75], 0.05)\n",
    "            iqr = quantiles[1] - quantiles[0]\n",
    "            lower_bound = quantiles[0] - 1.5 * iqr\n",
    "            upper_bound = quantiles[1] + 1.5 * iqr\n",
    "            bounds[col_name] = (lower_bound, upper_bound)\n",
    "        return bounds\n",
    "\n",
    "    # Calculate lower and upper bounds for each column\n",
    "    bounds = calculate_iqr_bounds(df, numeric_columns)\n",
    "\n",
    "    # Show the calculated bounds\n",
    "    for col_name, (lower_bound, upper_bound) in bounds.items():\n",
    "        print(f\"Column: {col_name}, Lower Bound: {lower_bound}, Upper Bound: {upper_bound}\")\n",
    "\n",
    "    # Now, you can decide whether to remove or transform outliers based on the calculated bounds.\n",
    "    # For example, you can filter the DataFrame to remove outliers:\n",
    "    filtered_data = parquet_data\n",
    "    for col_name, (lower_bound, upper_bound) in bounds.items():\n",
    "        filtered_data = filtered_data.filter((col(col_name) >= lower_bound) & (col(col_name) <= upper_bound))\n",
    "\n",
    "\n",
    "    # Optionally, you can transform outliers instead of removing them\n",
    "    # For example, apply a log transformation to \"DepDelay\" column\n",
    "    # Apply a log transformation to \"DepDelay\" column\n",
    "    filtered_data = filtered_data.withColumn(\"DepDelay\", log(col(\"DepDelay\") + 1))\n",
    "\n",
    "    # Data Type Conversion\n",
    "    filtered_data = filtered_data.withColumn(\"FlightDate\", unix_timestamp(col(\"FlightDate\")).cast(TimestampType()))\n",
    "\n",
    "    # Perform Transformation\n",
    "    # Create a new column 'TotalDelay' by adding DepDelay and ArrDelay\n",
    "    filtered_data = filtered_data.withColumn('TotalDelay', filtered_data['DepDelay'] + filtered_data['ArrDelay'])\n",
    "\n",
    "    # Show the filtered DataFrame\n",
    "    filtered_data.show(5)\n",
    "\n",
    "\n",
    "    # Close the Spark session for other operations\n",
    "    hdfs_reader.close_spark_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

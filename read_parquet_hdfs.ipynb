{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='9ee2484f4334', port=50075): Max retries exceeded with url: /webhdfs/v1/tmp/hive/root/Flight_Delay.parquet?op=CREATE&user.name=root&namenoderpcaddress=namenode:8020&overwrite=false&user.name=root (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7f14be5d84f0>: Failed to resolve '9ee2484f4334' ([Errno -3] Temporary failure in name resolution)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    204\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[1;32m    205\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    206\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[1;32m    207\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[39mraise\u001b[39;00m LocationParseError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mhost\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, label empty or too long\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    954\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 955\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[1;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:496\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     conn\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    497\u001b[0m         method,\n\u001b[1;32m    498\u001b[0m         url,\n\u001b[1;32m    499\u001b[0m         body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    500\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    501\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    502\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    503\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    504\u001b[0m         enforce_content_length\u001b[39m=\u001b[39;49menforce_content_length,\n\u001b[1;32m    505\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/connection.py:395\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 395\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders()\n\u001b[1;32m    397\u001b[0m \u001b[39m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/http/client.py:1277\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1277\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/http/client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1037\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m \n\u001b[1;32m   1041\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/http/client.py:975\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 975\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    976\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/connection.py:243\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    244\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tunnel_host:\n\u001b[1;32m    245\u001b[0m         \u001b[39m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/connection.py:210\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mraise\u001b[39;00m NameResolutionError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f14be5d84f0>: Failed to resolve '9ee2484f4334' ([Errno -3] Temporary failure in name resolution)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    842\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[0;32m--> 844\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    845\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    846\u001b[0m )\n\u001b[1;32m    847\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[39m=\u001b[39m error \u001b[39mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[39mfrom\u001b[39;00m \u001b[39mreason\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='9ee2484f4334', port=50075): Max retries exceeded with url: /webhdfs/v1/tmp/hive/root/Flight_Delay.parquet?op=CREATE&user.name=root&namenoderpcaddress=namenode:8020&overwrite=false&user.name=root (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7f14be5d84f0>: Failed to resolve '9ee2484f4334' ([Errno -3] Temporary failure in name resolution)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m hdfs_destination_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/tmp/hive/root\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Upload the local file to HDFS\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m hdfs_client\u001b[39m.\u001b[39;49mupload(hdfs_destination_path, local_file_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mlocal_file_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m uploaded to HDFS at \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mhdfs_destination_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/hdfs/client.py:654\u001b[0m, in \u001b[0;36mClient.upload\u001b[0;34m(self, hdfs_path, local_path, n_threads, temp_dir, chunk_size, progress, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m     _logger\u001b[39m.\u001b[39merror(\u001b[39m'\u001b[39m\u001b[39mUnable to cleanup temporary folder.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    653\u001b[0m   \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m    655\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m   \u001b[39mraise\u001b[39;00m err\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/hdfs/client.py:643\u001b[0m, in \u001b[0;36mClient.upload\u001b[0;34m(self, hdfs_path, local_path, n_threads, temp_dir, chunk_size, progress, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[39mif\u001b[39;00m n_threads \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    642\u001b[0m   \u001b[39mfor\u001b[39;00m path_tuple \u001b[39min\u001b[39;00m fpath_tuples:\n\u001b[0;32m--> 643\u001b[0m     _upload(path_tuple)\n\u001b[1;32m    644\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    645\u001b[0m   _map_async(n_threads, _upload, fpath_tuples)\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/hdfs/client.py:574\u001b[0m, in \u001b[0;36mClient.upload.<locals>._upload\u001b[0;34m(_path_tuple)\u001b[0m\n\u001b[1;32m    571\u001b[0m     _progress(_local_path, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    573\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(_local_path, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m reader:\n\u001b[0;32m--> 574\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite(_temp_path, wrap(reader, chunk_size, progress), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/hdfs/client.py:520\u001b[0m, in \u001b[0;36mClient.write\u001b[0;34m(self, hdfs_path, data, overwrite, permission, blocksize, replication, buffersize, append, encoding)\u001b[0m\n\u001b[1;32m    518\u001b[0m   \u001b[39mreturn\u001b[39;00m AsyncWriter(consumer)\n\u001b[1;32m    519\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m   consumer(data)\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/hdfs/client.py:509\u001b[0m, in \u001b[0;36mClient.write.<locals>.consumer\u001b[0;34m(_data)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconsumer\u001b[39m(_data):\n\u001b[1;32m    508\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Thread target.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m   res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    510\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39mif\u001b[39;49;00m append \u001b[39melse\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39mPUT\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    511\u001b[0m     url\u001b[39m=\u001b[39;49mloc,\n\u001b[1;32m    512\u001b[0m     data\u001b[39m=\u001b[39;49m(c\u001b[39m.\u001b[39;49mencode(encoding) \u001b[39mfor\u001b[39;49;00m c \u001b[39min\u001b[39;49;00m _data) \u001b[39mif\u001b[39;49;00m encoding \u001b[39melse\u001b[39;49;00m _data,\n\u001b[1;32m    513\u001b[0m   )\n\u001b[1;32m    514\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m res:\n\u001b[1;32m    515\u001b[0m     \u001b[39mraise\u001b[39;00m _to_error(res)\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/hdfs/client.py:209\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_request\u001b[39m(\u001b[39mself\u001b[39m, method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    200\u001b[0m \u001b[39m  \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Send request to WebHDFS API.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[39m  :param method: HTTP verb.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \n\u001b[1;32m    208\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    210\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    211\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    212\u001b[0m     timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeout,\n\u001b[1;32m    213\u001b[0m     headers\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mcontent-type\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mapplication/octet-stream\u001b[39;49m\u001b[39m'\u001b[39;49m}, \u001b[39m# For HttpFS.\u001b[39;49;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    215\u001b[0m   )\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='9ee2484f4334', port=50075): Max retries exceeded with url: /webhdfs/v1/tmp/hive/root/Flight_Delay.parquet?op=CREATE&user.name=root&namenoderpcaddress=namenode:8020&overwrite=false&user.name=root (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7f14be5d84f0>: Failed to resolve '9ee2484f4334' ([Errno -3] Temporary failure in name resolution)\"))"
     ]
    }
   ],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "# Connect to HDFS (adjust the URL to match your Hadoop cluster)\n",
    "hdfs_client = InsecureClient(\"http://localhost:9870\", user=\"anna\")\n",
    "\n",
    "# Local file path\n",
    "local_file_path = \"/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/data/Flight_data/Flight_Delay.parquet\"\n",
    "\n",
    "# HDFS destination path\n",
    "hdfs_destination_path = \"/user/anna/flight_data\"\n",
    "\n",
    "# Upload the local file to HDFS\n",
    "hdfs_client.upload(hdfs_destination_path, local_file_path)\n",
    "\n",
    "print(f\"File '{local_file_path}' uploaded to HDFS at '{hdfs_destination_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=9870): Max retries exceeded with url: /webhdfs/v1/user/anna/flight_data?user.name=anna&op=LISTSTATUS (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f14d443ea40>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    204\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[1;32m    205\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    206\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[1;32m    207\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     74\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:496\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     conn\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    497\u001b[0m         method,\n\u001b[1;32m    498\u001b[0m         url,\n\u001b[1;32m    499\u001b[0m         body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    500\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    501\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    502\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    503\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    504\u001b[0m         enforce_content_length\u001b[39m=\u001b[39;49menforce_content_length,\n\u001b[1;32m    505\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/connection.py:395\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 395\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders()\n\u001b[1;32m    397\u001b[0m \u001b[39m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/http/client.py:1277\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1277\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/http/client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1037\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m \n\u001b[1;32m   1041\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/http/client.py:975\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 975\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    976\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/connection.py:243\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    244\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tunnel_host:\n\u001b[1;32m    245\u001b[0m         \u001b[39m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/connection.py:218\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    219\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f14d443ea40>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    842\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[0;32m--> 844\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    845\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    846\u001b[0m )\n\u001b[1;32m    847\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[39m=\u001b[39m error \u001b[39mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[39mfrom\u001b[39;00m \u001b[39mreason\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=9870): Max retries exceeded with url: /webhdfs/v1/user/anna/flight_data?user.name=anna&op=LISTSTATUS (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f14d443ea40>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m hdfs_destination_path2 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/user/anna/flight_data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Upload the local file to HDFS\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m hdfs_client\u001b[39m.\u001b[39;49mupload(hdfs_destination_path2, local_file_path2)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mlocal_file_path2\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m uploaded to HDFS at \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mhdfs_destination_path2\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/hdfs/client.py:580\u001b[0m, in \u001b[0;36mClient.upload\u001b[0;34m(self, hdfs_path, local_path, n_threads, temp_dir, chunk_size, progress, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m temp_path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 580\u001b[0m   statuses \u001b[39m=\u001b[39m [status \u001b[39mfor\u001b[39;00m _, status \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlist(hdfs_path, status\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)]\n\u001b[1;32m    581\u001b[0m \u001b[39mexcept\u001b[39;00m HdfsError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    582\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mnot a directory\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m err\u001b[39m.\u001b[39mmessage:\n\u001b[1;32m    583\u001b[0m     \u001b[39m# Remote path is a normal file.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/hdfs/client.py:1118\u001b[0m, in \u001b[0;36mClient.list\u001b[0;34m(self, hdfs_path, status)\u001b[0m\n\u001b[1;32m   1116\u001b[0m _logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mListing \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m, hdfs_path)\n\u001b[1;32m   1117\u001b[0m hdfs_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve(hdfs_path)\n\u001b[0;32m-> 1118\u001b[0m statuses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_list_status(hdfs_path)\u001b[39m.\u001b[39mjson()[\u001b[39m'\u001b[39m\u001b[39mFileStatuses\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mFileStatus\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m   1119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(statuses) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m (\n\u001b[1;32m   1120\u001b[0m   \u001b[39mnot\u001b[39;00m statuses[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mpathSuffix\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus(hdfs_path)[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mFILE\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1121\u001b[0m   \u001b[39m# HttpFS behaves incorrectly here, we sometimes need an extra call to\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m   \u001b[39m# make sure we always identify if we are dealing with a file.\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m ):\n\u001b[1;32m   1124\u001b[0m   \u001b[39mraise\u001b[39;00m HdfsError(\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m is not a directory.\u001b[39m\u001b[39m'\u001b[39m, hdfs_path)\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/hdfs/client.py:125\u001b[0m, in \u001b[0;36m_Request.to_method.<locals>.api_handler\u001b[0;34m(client, hdfs_path, data, strict, **params)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(client\u001b[39m.\u001b[39m_urls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    124\u001b[0m   _logger\u001b[39m.\u001b[39mwarning(\u001b[39m'\u001b[39m\u001b[39mNo reachable host, raising last error.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 125\u001b[0m \u001b[39mraise\u001b[39;00m err\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/hdfs/client.py:102\u001b[0m, in \u001b[0;36m_Request.to_method.<locals>.api_handler\u001b[0;34m(client, hdfs_path, data, strict, **params)\u001b[0m\n\u001b[1;32m    100\u001b[0m err \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m   res \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    103\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    104\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    105\u001b[0m     data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    106\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    107\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs\n\u001b[1;32m    108\u001b[0m   )\n\u001b[1;32m    109\u001b[0m \u001b[39mexcept\u001b[39;00m (rq\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mReadTimeout, rq\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectTimeout,\n\u001b[1;32m    110\u001b[0m         rq\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectionError) \u001b[39mas\u001b[39;00m retriable_err:\n\u001b[1;32m    111\u001b[0m   err \u001b[39m=\u001b[39m retriable_err \u001b[39m# Retry.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/hdfs/client.py:209\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_request\u001b[39m(\u001b[39mself\u001b[39m, method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    200\u001b[0m \u001b[39m  \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Send request to WebHDFS API.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[39m  :param method: HTTP verb.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \n\u001b[1;32m    208\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    210\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    211\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    212\u001b[0m     timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeout,\n\u001b[1;32m    213\u001b[0m     headers\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mcontent-type\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mapplication/octet-stream\u001b[39;49m\u001b[39m'\u001b[39;49m}, \u001b[39m# For HttpFS.\u001b[39;49;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    215\u001b[0m   )\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=9870): Max retries exceeded with url: /webhdfs/v1/user/anna/flight_data?user.name=anna&op=LISTSTATUS (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f14d443ea40>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "# Connect to HDFS (adjust the URL to match your Hadoop cluster)\n",
    "hdfs_client = InsecureClient(\"http://localhost:9870\", user=\"anna\")\n",
    "\n",
    "# Local file path\n",
    "local_file_path2 = \"/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/data/Flight_data/features_added.parquet\"\n",
    "\n",
    "# HDFS destination path\n",
    "hdfs_destination_path2 = \"/user/anna/flight_data\"\n",
    "\n",
    "# Upload the local file to HDFS\n",
    "hdfs_client.upload(hdfs_destination_path2, local_file_path2)\n",
    "\n",
    "print(f\"File '{local_file_path2}' uploaded to HDFS at '{hdfs_destination_path2}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/13 19:23:22 WARN Utils: Your hostname, LAPTOP-PCQSOCPF resolves to a loopback address: 127.0.1.1; using 172.24.131.235 instead (on interface eth0)\n",
      "23/09/13 19:23:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/13 19:23:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/13 19:24:02 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----------+-------------------------+--------------+--------------+----------+-------+--------+---------------+-------+---------+--------+------+----------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+-----------------+\n",
      "|Year|Month|DayofMonth|FlightDate|Marketing_Airline_Network|OriginCityName|  DestCityName|CRSDepTime|DepTime|DepDelay|DepDelayMinutes|TaxiOut|WheelsOff|WheelsOn|TaxiIn|CRSArrTime|ArrTime|ArrDelay|ArrDelayMinutes|CRSElapsedTime|ActualElapsedTime|AirTime|Distance|DistanceGroup|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|__index_level_0__|\n",
      "+----+-----+----------+----------+-------------------------+--------------+--------------+----------+-------+--------+---------------+-------+---------+--------+------+----------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+-----------------+\n",
      "|2018|    1|        15|2018-01-15|                       UA|    Newark, NJ|Charleston, SC|      1845| 1928.0|    43.0|           43.0|   35.0|   2003.0|  2145.0|   4.0|      2108| 2149.0|    41.0|           41.0|         143.0|            141.0|  102.0|   628.0|            3|        41.0|         0.0|     0.0|          0.0|              0.0|                1|\n",
      "|2018|    1|        16|2018-01-16|                       UA|    Newark, NJ|Charleston, SC|      1835| 1956.0|    81.0|           81.0|   18.0|   2014.0|  2202.0|   5.0|      2058| 2207.0|    69.0|           69.0|         143.0|            131.0|  108.0|   628.0|            3|        67.0|         0.0|     0.0|          0.0|              2.0|                2|\n",
      "|2018|    1|        17|2018-01-17|                       UA|    Newark, NJ|Charleston, SC|      1835| 1836.0|     1.0|            1.0|   19.0|   1855.0|  2048.0|   6.0|      2058| 2054.0|    -4.0|            0.0|         143.0|            138.0|  113.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|                3|\n",
      "|2018|    1|        18|2018-01-18|                       UA|    Newark, NJ|Charleston, SC|      1845| 1844.0|    -1.0|            0.0|   36.0|   1920.0|  2052.0|   6.0|      2108| 2058.0|   -10.0|            0.0|         143.0|            134.0|   92.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|                4|\n",
      "|2018|    1|        20|2018-01-20|                       UA|    Newark, NJ|Charleston, SC|      1835| 1829.0|    -6.0|            0.0|   22.0|   1851.0|  2020.0|   6.0|      2058| 2026.0|   -32.0|            0.0|         143.0|            117.0|   89.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|                6|\n",
      "+----+-----+----------+----------+-------------------------+--------------+--------------+----------+-------+--------+---------------+-------+---------+--------+------+----------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "# Connect to HDFS (adjust the URL to match your Hadoop cluster)\n",
    "hdfs_client = InsecureClient(\"http://localhost:9870\", user=\"anna\")\n",
    "\n",
    "# Create a SparkConf object and set Hadoop configuration properties\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"MySparkApp\") \\\n",
    "    .setMaster(\"local[2]\")  # Use your Spark cluster's master URL if not running locally\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.defaultFS\", \"http://localhost:9870\")\n",
    "conf.set(\"spark.hadoop.yarn.resourcemanager.hostname\", \"http://localhost:8088/\")\n",
    "\n",
    "# Create a SparkSession with the configured conf object\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Now you can use Spark to access HDFS\n",
    "parquet_file_path = \"hdfs://localhost:9000/user/anna/flight_data/Flight_Delay.parquet\"\n",
    "df = spark.read.parquet(parquet_file_path)\n",
    "df.show(5)\n",
    "\n",
    "# # Now you can use Spark to access HDFS\n",
    "# parquet_file_path2 = \"hdfs://localhost:9000//user/anna/flight_data/features_added.parquet\"\n",
    "# df2 = spark.read.parquet(parquet_file_path2)\n",
    "# df2.show(5)\n",
    "\n",
    "# Don't forget to stop the SparkSession when done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/13 19:40:15 ERROR SparkContext: Error initializing SparkContext.\n",
      "org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\n",
      "\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)\n",
      "23/09/13 19:40:15 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to send shutdown message before the AM has registered!\n",
      "23/09/13 19:40:15 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "23/09/13 19:40:15 WARN MetricsSystem: Stopping a MetricsSystem that is not running\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m conf\u001b[39m.\u001b[39mset(\u001b[39m\"\u001b[39m\u001b[39mspark.hadoop.yarn.resourcemanager.hostname\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhttp://localhost:8088/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Create a SparkContext with the configured conf object\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m sc \u001b[39m=\u001b[39m SparkContext(conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Create a SparkSession (optional, for Spark SQL)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/read_parquet_hdfs.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/pyspark/context.py:200\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    198\u001b[0m SparkContext\u001b[39m.\u001b[39m_ensure_initialized(\u001b[39mself\u001b[39m, gateway\u001b[39m=\u001b[39mgateway, conf\u001b[39m=\u001b[39mconf)\n\u001b[1;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_init(\n\u001b[1;32m    201\u001b[0m         master,\n\u001b[1;32m    202\u001b[0m         appName,\n\u001b[1;32m    203\u001b[0m         sparkHome,\n\u001b[1;32m    204\u001b[0m         pyFiles,\n\u001b[1;32m    205\u001b[0m         environment,\n\u001b[1;32m    206\u001b[0m         batchSize,\n\u001b[1;32m    207\u001b[0m         serializer,\n\u001b[1;32m    208\u001b[0m         conf,\n\u001b[1;32m    209\u001b[0m         jsc,\n\u001b[1;32m    210\u001b[0m         profiler_cls,\n\u001b[1;32m    211\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    213\u001b[0m     )\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[39m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop()\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/pyspark/context.py:287\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment[\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    286\u001b[0m \u001b[39m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc \u001b[39m=\u001b[39m jsc \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_context(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conf\u001b[39m.\u001b[39;49m_jconf)\n\u001b[1;32m    288\u001b[0m \u001b[39m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf \u001b[39m=\u001b[39m SparkConf(_jconf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39mconf())\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/pyspark/context.py:417\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[39mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mJavaSparkContext(jconf)\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1588\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[1;32m   1590\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/.venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "# Connect to HDFS (adjust the URL to match your Hadoop cluster)\n",
    "hdfs_client = InsecureClient(\"http://localhost:9870\", user=\"anna\")\n",
    "\n",
    "# Create a SparkConf object\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"MySparkApp\") \\\n",
    "    .setMaster(\"yarn\")  # Use the cluster manager you're using (e.g., yarn, mesos, etc.)\n",
    "\n",
    "# Configure the number of executors, executor memory, and other properties\n",
    "conf.set(\"spark.executor.instances\", \"4\")  # Number of executor instances\n",
    "conf.set(\"spark.executor.memory\", \"2g\")     # Memory per executor\n",
    "conf.set(\"spark.executor.cores\", \"2\")       # Cores per executor\n",
    "conf.set(\"spark.driver.memory\", \"2g\")       # Memory for the driver (main application)\n",
    "conf.set(\"spark.hadoop.fs.defaultFS\", \"http://localhost:9870\")\n",
    "conf.set(\"spark.hadoop.yarn.resourcemanager.hostname\", \"http://localhost:8088/\")\n",
    "\n",
    "# Create a SparkContext with the configured conf object\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Create a SparkSession (optional, for Spark SQL)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Now you can use Spark for processing\n",
    "\n",
    "# Now you can use Spark to access HDFS\n",
    "parquet_file_path = \"hdfs://localhost:9000/user/anna/flight_data/Flight_Delay.parquet\"\n",
    "df = spark.read.parquet(parquet_file_path)\n",
    "df.show(5)\n",
    "\n",
    "# Now you can use Spark to access HDFS\n",
    "parquet_file_path2 = \"hdfs://localhost:9000//user/anna/flight_data/features_added.parquet\"\n",
    "df2 = spark.read.parquet(parquet_file_path2)\n",
    "df2.show(5)\n",
    "\n",
    "# Don't forget to stop the SparkSession when done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/13 19:57:27 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-------------------+-------------------------+--------------+--------------+-------+--------+---------------+-------+------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+---------+--------+----------------+--------------+---------------+-------------+----------------+--------------+--------------+------------+-----------------+----------------+-----------------+---------------+-------------------+-----------------+\n",
      "|Year|Month|DayofMonth|         FlightDate|Marketing_Airline_Network|OriginCityName|  DestCityName|DepTime|DepDelay|DepDelayMinutes|TaxiOut|TaxiIn|ArrTime|ArrDelay|ArrDelayMinutes|CRSElapsedTime|ActualElapsedTime|AirTime|Distance|DistanceGroup|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|DayofWeek|Holidays|CRSDepTimeMinute|CRSDepTimeHour|WheelsOffMinute|WheelsOffHour|CRSArrTimeMinute|CRSArrTimeHour|WheelsOnMinute|WheelsOnHour|CRSDepTimeHourDis|WheelsOffHourDis|CRSArrTimeHourDis|WheelsOnHourDis|CRSElapsedTimeGorup|__index_level_0__|\n",
      "+----+-----+----------+-------------------+-------------------------+--------------+--------------+-------+--------+---------------+-------+------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+---------+--------+----------------+--------------+---------------+-------------+----------------+--------------+--------------+------------+-----------------+----------------+-----------------+---------------+-------------------+-----------------+\n",
      "|2018|    1|        15|2018-01-15 07:00:00|                       UA|    Newark, NJ|Charleston, SC| 1928.0|    43.0|           43.0|   35.0|   4.0| 2149.0|    41.0|           41.0|         143.0|            141.0|  102.0|   628.0|            3|        41.0|         0.0|     0.0|          0.0|              0.0|        0|    true|              45|            18|              3|           20|               8|            21|            45|          21|          Evening|         Evening|            Night|          Night|                  3|                1|\n",
      "|2018|    1|        16|2018-01-16 07:00:00|                       UA|    Newark, NJ|Charleston, SC| 1956.0|    81.0|           81.0|   18.0|   5.0| 2207.0|    69.0|           69.0|         143.0|            131.0|  108.0|   628.0|            3|        67.0|         0.0|     0.0|          0.0|              2.0|        1|   false|              35|            18|             14|           20|              58|            20|             2|          22|          Evening|         Evening|          Evening|          Night|                  3|                2|\n",
      "|2018|    1|        17|2018-01-17 07:00:00|                       UA|    Newark, NJ|Charleston, SC| 1836.0|     1.0|            1.0|   19.0|   6.0| 2054.0|    -4.0|            0.0|         143.0|            138.0|  113.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|        2|   false|              35|            18|             55|           18|              58|            20|            48|          20|          Evening|         Evening|          Evening|        Evening|                  3|                3|\n",
      "|2018|    1|        18|2018-01-18 07:00:00|                       UA|    Newark, NJ|Charleston, SC| 1844.0|    -1.0|            0.0|   36.0|   6.0| 2058.0|   -10.0|            0.0|         143.0|            134.0|   92.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|        3|   false|              45|            18|             20|           19|               8|            21|            52|          20|          Evening|         Evening|            Night|        Evening|                  3|                4|\n",
      "|2018|    1|        20|2018-01-20 07:00:00|                       UA|    Newark, NJ|Charleston, SC| 1829.0|    -6.0|            0.0|   22.0|   6.0| 2026.0|   -32.0|            0.0|         143.0|            117.0|   89.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|        5|   false|              35|            18|             51|           18|              58|            20|            20|          20|          Evening|         Evening|          Evening|        Evening|                  3|                6|\n",
      "+----+-----+----------+-------------------+-------------------------+--------------+--------------+-------+--------+---------------+-------+------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+---------+--------+----------------+--------------+---------------+-------------+----------------+--------------+--------------+------------+-----------------+----------------+-----------------+---------------+-------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "# Connect to HDFS (adjust the URL to match your Hadoop cluster)\n",
    "hdfs_client = InsecureClient(\"http://localhost:9870\", user=\"anna\")\n",
    "\n",
    "# Create a SparkConf object and set Hadoop configuration properties\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"MySparkApp2\") \\\n",
    "    .setMaster(\"local[2]\")  # Use your Spark cluster's master URL if not running locally\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.defaultFS\", \"http://localhost:9870\")\n",
    "conf.set(\"spark.hadoop.yarn.resourcemanager.hostname\", \"http://localhost:8088/\")\n",
    "\n",
    "# Create a SparkSession with the configured conf object\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Now you can use Spark to access HDFS\n",
    "parquet_file_path2 = \"hdfs://localhost:9000//user/anna/flight_data/features_added.parquet\"\n",
    "df2 = spark.read.parquet(parquet_file_path2)\n",
    "df2.show(5)\n",
    "\n",
    "# Don't forget to stop the SparkSession when done\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Month: long (nullable = true)\n",
      " |-- DayofMonth: long (nullable = true)\n",
      " |-- FlightDate: string (nullable = true)\n",
      " |-- Marketing_Airline_Network: string (nullable = true)\n",
      " |-- OriginCityName: string (nullable = true)\n",
      " |-- DestCityName: string (nullable = true)\n",
      " |-- CRSDepTime: long (nullable = true)\n",
      " |-- DepTime: double (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- DepDelayMinutes: double (nullable = true)\n",
      " |-- TaxiOut: double (nullable = true)\n",
      " |-- WheelsOff: double (nullable = true)\n",
      " |-- WheelsOn: double (nullable = true)\n",
      " |-- TaxiIn: double (nullable = true)\n",
      " |-- CRSArrTime: long (nullable = true)\n",
      " |-- ArrTime: double (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- ArrDelayMinutes: double (nullable = true)\n",
      " |-- CRSElapsedTime: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- DistanceGroup: long (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "# Connect to HDFS (adjust the URL to match your Hadoop cluster)\n",
    "hdfs_client = InsecureClient(\"http://localhost:9870\", user=\"anna\")\n",
    "\n",
    "# Create a SparkConf object and set Hadoop configuration properties\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"MySparkApp\") \\\n",
    "    .setMaster(\"local[2]\")  # Use your Spark cluster's master URL if not running locally\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\")\n",
    "conf.set(\"spark.hadoop.yarn.resourcemanager.hostname\", \"localhost\")\n",
    "\n",
    "# Create a SparkSession with the configured conf object\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Now you can use Spark to access HDFS\n",
    "parquet_file_path = \"hdfs://localhost:9000/user/anna/flight_data/Flight_Delay.parquet\"\n",
    "\n",
    "# Read the Parquet file\n",
    "df = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Print the schema (column names and data types)\n",
    "df.printSchema()\n",
    "\n",
    "# Don't forget to stop the SparkSession when done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Month: long (nullable = true)\n",
      " |-- DayofMonth: long (nullable = true)\n",
      " |-- FlightDate: timestamp_ntz (nullable = true)\n",
      " |-- Marketing_Airline_Network: string (nullable = true)\n",
      " |-- OriginCityName: string (nullable = true)\n",
      " |-- DestCityName: string (nullable = true)\n",
      " |-- DepTime: double (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- DepDelayMinutes: double (nullable = true)\n",
      " |-- TaxiOut: double (nullable = true)\n",
      " |-- TaxiIn: double (nullable = true)\n",
      " |-- ArrTime: double (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- ArrDelayMinutes: double (nullable = true)\n",
      " |-- CRSElapsedTime: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- DistanceGroup: long (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      " |-- DayofWeek: long (nullable = true)\n",
      " |-- Holidays: boolean (nullable = true)\n",
      " |-- CRSDepTimeMinute: integer (nullable = true)\n",
      " |-- CRSDepTimeHour: integer (nullable = true)\n",
      " |-- WheelsOffMinute: integer (nullable = true)\n",
      " |-- WheelsOffHour: integer (nullable = true)\n",
      " |-- CRSArrTimeMinute: integer (nullable = true)\n",
      " |-- CRSArrTimeHour: integer (nullable = true)\n",
      " |-- WheelsOnMinute: integer (nullable = true)\n",
      " |-- WheelsOnHour: integer (nullable = true)\n",
      " |-- CRSDepTimeHourDis: string (nullable = true)\n",
      " |-- WheelsOffHourDis: string (nullable = true)\n",
      " |-- CRSArrTimeHourDis: string (nullable = true)\n",
      " |-- WheelsOnHourDis: string (nullable = true)\n",
      " |-- CRSElapsedTimeGorup: long (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "# Connect to HDFS (adjust the URL to match your Hadoop cluster)\n",
    "hdfs_client = InsecureClient(\"http://localhost:9870\", user=\"anna\")\n",
    "\n",
    "# Create a SparkConf object and set Hadoop configuration properties\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"MySparkApp\") \\\n",
    "    .setMaster(\"local[2]\")  # Use your Spark cluster's master URL if not running locally\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\")\n",
    "conf.set(\"spark.hadoop.yarn.resourcemanager.hostname\", \"localhost\")\n",
    "\n",
    "# Create a SparkSession with the configured conf object\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Now you can use Spark to access HDFS\n",
    "parquet_file_path2 = \"hdfs://localhost:9000/user/anna/flight_data/features_added.parquet\"\n",
    "\n",
    "# Read the Parquet file\n",
    "df2 = spark.read.parquet(parquet_file_path2)\n",
    "\n",
    "# Print the schema (column names and data types)\n",
    "df2.printSchema()\n",
    "\n",
    "# Don't forget to stop the SparkSession when done\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/data/Flight_data/Flight_Delay.parquet' uploaded to HDFS at '/user/anna/flight_data'\n"
     ]
    }
   ],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "# Connect to HDFS (adjust the URL to match your Hadoop cluster)\n",
    "hdfs_client = InsecureClient(\"http://localhost:9870\", user=\"anna\")\n",
    "\n",
    "# Local file path\n",
    "local_file_path = \"/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/data/Flight_data/Flight_Delay.parquet\"\n",
    "\n",
    "# HDFS destination path\n",
    "hdfs_destination_path = \"/user/anna/flight_data\"\n",
    "\n",
    "# Upload the local file to HDFS\n",
    "hdfs_client.upload(hdfs_destination_path, local_file_path)\n",
    "\n",
    "print(f\"File '{local_file_path}' uploaded to HDFS at '{hdfs_destination_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/data/Flight_data/features_added.parquet' uploaded to HDFS at '/user/anna/flight_data'\n"
     ]
    }
   ],
   "source": [
    "# Local file path\n",
    "local_file_path2 = \"/mnt/c/linux/ETL_Project/Python_ETL_Adv_Works/data/Flight_data/features_added.parquet\"\n",
    "\n",
    "# HDFS destination path\n",
    "hdfs_destination_path2 = \"/user/anna/flight_data\"\n",
    "\n",
    "# Upload the local file to HDFS\n",
    "hdfs_client.upload(hdfs_destination_path2, local_file_path2)\n",
    "\n",
    "print(f\"File '{local_file_path2}' uploaded to HDFS at '{hdfs_destination_path2}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/11 14:45:01 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----------+-------------------------+--------------+--------------+----------+-------+--------+---------------+-------+---------+--------+------+----------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+-----------------+\n",
      "|Year|Month|DayofMonth|FlightDate|Marketing_Airline_Network|OriginCityName|  DestCityName|CRSDepTime|DepTime|DepDelay|DepDelayMinutes|TaxiOut|WheelsOff|WheelsOn|TaxiIn|CRSArrTime|ArrTime|ArrDelay|ArrDelayMinutes|CRSElapsedTime|ActualElapsedTime|AirTime|Distance|DistanceGroup|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|__index_level_0__|\n",
      "+----+-----+----------+----------+-------------------------+--------------+--------------+----------+-------+--------+---------------+-------+---------+--------+------+----------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+-----------------+\n",
      "|2018|    1|        15|2018-01-15|                       UA|    Newark, NJ|Charleston, SC|      1845| 1928.0|    43.0|           43.0|   35.0|   2003.0|  2145.0|   4.0|      2108| 2149.0|    41.0|           41.0|         143.0|            141.0|  102.0|   628.0|            3|        41.0|         0.0|     0.0|          0.0|              0.0|                1|\n",
      "|2018|    1|        16|2018-01-16|                       UA|    Newark, NJ|Charleston, SC|      1835| 1956.0|    81.0|           81.0|   18.0|   2014.0|  2202.0|   5.0|      2058| 2207.0|    69.0|           69.0|         143.0|            131.0|  108.0|   628.0|            3|        67.0|         0.0|     0.0|          0.0|              2.0|                2|\n",
      "|2018|    1|        17|2018-01-17|                       UA|    Newark, NJ|Charleston, SC|      1835| 1836.0|     1.0|            1.0|   19.0|   1855.0|  2048.0|   6.0|      2058| 2054.0|    -4.0|            0.0|         143.0|            138.0|  113.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|                3|\n",
      "|2018|    1|        18|2018-01-18|                       UA|    Newark, NJ|Charleston, SC|      1845| 1844.0|    -1.0|            0.0|   36.0|   1920.0|  2052.0|   6.0|      2108| 2058.0|   -10.0|            0.0|         143.0|            134.0|   92.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|                4|\n",
      "|2018|    1|        20|2018-01-20|                       UA|    Newark, NJ|Charleston, SC|      1835| 1829.0|    -6.0|            0.0|   22.0|   1851.0|  2020.0|   6.0|      2058| 2026.0|   -32.0|            0.0|         143.0|            117.0|   89.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|                6|\n",
      "+----+-----+----------+----------+-------------------------+--------------+--------------+----------+-------+--------+---------------+-------+---------+--------+------+----------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkConf object and set Hadoop configuration properties\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"MySparkApp\") \\\n",
    "    .setMaster(\"local[2]\")  # Use your Spark cluster's master URL if not running locally\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.defaultFS\", \"http://localhost:9870\")\n",
    "conf.set(\"spark.hadoop.yarn.resourcemanager.hostname\", \"http://localhost:8088/\")\n",
    "\n",
    "# Create a SparkSession with the configured conf object\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Now you can use Spark to access HDFS\n",
    "parquet_file_path = \"hdfs://localhost:9000/user/anna/flight_data/Flight_Delay.parquet\"\n",
    "df = spark.read.parquet(parquet_file_path)\n",
    "df.show(5)\n",
    "\n",
    "# Don't forget to stop the SparkSession when done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-------------------+-------------------------+--------------+--------------+-------+--------+---------------+-------+------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+---------+--------+----------------+--------------+---------------+-------------+----------------+--------------+--------------+------------+-----------------+----------------+-----------------+---------------+-------------------+-----------------+\n",
      "|Year|Month|DayofMonth|         FlightDate|Marketing_Airline_Network|OriginCityName|  DestCityName|DepTime|DepDelay|DepDelayMinutes|TaxiOut|TaxiIn|ArrTime|ArrDelay|ArrDelayMinutes|CRSElapsedTime|ActualElapsedTime|AirTime|Distance|DistanceGroup|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|DayofWeek|Holidays|CRSDepTimeMinute|CRSDepTimeHour|WheelsOffMinute|WheelsOffHour|CRSArrTimeMinute|CRSArrTimeHour|WheelsOnMinute|WheelsOnHour|CRSDepTimeHourDis|WheelsOffHourDis|CRSArrTimeHourDis|WheelsOnHourDis|CRSElapsedTimeGorup|__index_level_0__|\n",
      "+----+-----+----------+-------------------+-------------------------+--------------+--------------+-------+--------+---------------+-------+------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+---------+--------+----------------+--------------+---------------+-------------+----------------+--------------+--------------+------------+-----------------+----------------+-----------------+---------------+-------------------+-----------------+\n",
      "|2018|    1|        15|2018-01-15 00:00:00|                       UA|    Newark, NJ|Charleston, SC| 1928.0|    43.0|           43.0|   35.0|   4.0| 2149.0|    41.0|           41.0|         143.0|            141.0|  102.0|   628.0|            3|        41.0|         0.0|     0.0|          0.0|              0.0|        0|    true|              45|            18|              3|           20|               8|            21|            45|          21|          Evening|         Evening|            Night|          Night|                  3|                1|\n",
      "|2018|    1|        16|2018-01-16 00:00:00|                       UA|    Newark, NJ|Charleston, SC| 1956.0|    81.0|           81.0|   18.0|   5.0| 2207.0|    69.0|           69.0|         143.0|            131.0|  108.0|   628.0|            3|        67.0|         0.0|     0.0|          0.0|              2.0|        1|   false|              35|            18|             14|           20|              58|            20|             2|          22|          Evening|         Evening|          Evening|          Night|                  3|                2|\n",
      "|2018|    1|        17|2018-01-17 00:00:00|                       UA|    Newark, NJ|Charleston, SC| 1836.0|     1.0|            1.0|   19.0|   6.0| 2054.0|    -4.0|            0.0|         143.0|            138.0|  113.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|        2|   false|              35|            18|             55|           18|              58|            20|            48|          20|          Evening|         Evening|          Evening|        Evening|                  3|                3|\n",
      "|2018|    1|        18|2018-01-18 00:00:00|                       UA|    Newark, NJ|Charleston, SC| 1844.0|    -1.0|            0.0|   36.0|   6.0| 2058.0|   -10.0|            0.0|         143.0|            134.0|   92.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|        3|   false|              45|            18|             20|           19|               8|            21|            52|          20|          Evening|         Evening|            Night|        Evening|                  3|                4|\n",
      "|2018|    1|        20|2018-01-20 00:00:00|                       UA|    Newark, NJ|Charleston, SC| 1829.0|    -6.0|            0.0|   22.0|   6.0| 2026.0|   -32.0|            0.0|         143.0|            117.0|   89.0|   628.0|            3|         0.0|         0.0|     0.0|          0.0|              0.0|        5|   false|              35|            18|             51|           18|              58|            20|            20|          20|          Evening|         Evening|          Evening|        Evening|                  3|                6|\n",
      "+----+-----+----------+-------------------+-------------------------+--------------+--------------+-------+--------+---------------+-------+------+-------+--------+---------------+--------------+-----------------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+---------+--------+----------------+--------------+---------------+-------------+----------------+--------------+--------------+------------+-----------------+----------------+-----------------+---------------+-------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkConf object and set Hadoop configuration properties\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"MySparkApp\") \\\n",
    "    .setMaster(\"local[2]\")  # Use your Spark cluster's master URL if not running locally\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.defaultFS\", \"http://localhost:9870\")\n",
    "conf.set(\"spark.hadoop.yarn.resourcemanager.hostname\", \"http://localhost:8088/\")\n",
    "\n",
    "# Create a SparkSession with the configured conf object\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Now you can use Spark to access HDFS\n",
    "parquet_file_path2 = \"hdfs://localhost:9000//user/anna/flight_data/features_added.parquet\"\n",
    "df = spark.read.parquet(parquet_file_path2)\n",
    "df.show(5)\n",
    "\n",
    "# Don't forget to stop the SparkSession when done\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
